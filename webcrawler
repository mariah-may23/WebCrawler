#!/usr/bin/env python3

import cgi
import socket
import sys
import ssl
from collections import deque
from html.parser import HTMLParser

# set to keep a track of unique URLs already crawled
visited_links = set()
# a queue to keep track of URLs to be crawled
crawl_pages = deque()
# set of unique secret flags found in the pages
secret_flags = set()

'''
The FakebookHTMLParser extends the HTML Parser to parse through the response for 'a' tags and 'h2' tags in search of 
more URLs and/or secret flags respectively.
It also extracts the content of the 'h2' flags to get the content in between the tag i.e. the secret flag.
'''


class FakebookHTMLParser(HTMLParser):
    is_h2 = False
    acc_data = ''

    def handle_starttag(self, tag, attrs):
        if tag == 'a':  # only looking at the links tags
            for attrs in attrs:  # attrs will be the href and the link in a tuple
                link = attrs[1]  # the fakebook link or accounts/logout link
                if link.startswith("/fakebook"):  # only want to fakebook pages link
                    crawl_pages.append(link)  # add to the queue of URLs to be crawled

        if tag == "h2":  # only interested in getting secret flags under h2
            for attributes in attrs:
                if "secret_flag" in attributes:  # if secret flags under the attributes then set flag to found-- true
                    self.is_h2 = True

    def handle_endtag(self, tag):
        if tag == 'h2':  # after finding end tag, set found flag to false
            self.is_h2 = False
            if self.acc_data != '':  # if data has content then parse it to print and get the secret flag
                flag = self.acc_data.split()
                secret_flags.add(flag[1])  # add secret flag to the set of flags
                print(flag[1])
            self.acc_data = ''  # set data to empty string for next secret flag

    def handle_data(self, data):  # this collects the content between the h2 tag
        if self.is_h2:
            self.acc_data += data


'''
This method parses the command line arguments to get the username and password for the user in question.
It also throws appropriate errors for missing arguments.
'''


def parse_cmd_line():
    args_cmd = sys.argv
    username = ""
    password = ""

    try:
        file_name = sys.argv[0]
        username = sys.argv[1]
        password = sys.argv[2]
        return username, password

    except:
        if username == "":
            sys.exit("Please provide appropriate user name.")
        if password == "":
            sys.exit("Please provide appropriate password.")


'''
This method creates a TLS wrapped socket to create a connection to http server.
'''


def create_socket():
    port = 443
    host_name = 'project2.5700.network'
    # http is based on TCP so create a TCP/IP socket
    # building connection
    try:
        context = ssl.create_default_context()
        sock = socket.create_connection((host_name, port))
        wrapped_socket = context.wrap_socket(sock, server_hostname='project2.5700.network')
        return wrapped_socket
    except socket.error:
        sys.exit("Connection error.")


def send_get_request(path, sock, host, cookie1=None, cookie2=None):
    if cookie1 is None and cookie2 is None:  # no cookies given
        msg = f'GET {path} HTTP/1.1\r\n' \
              f'{host}\r\n\r\n'
        # print(msg)
    elif cookie2 is None:  # only cookie 1 given
        msg = f'GET {path} HTTP/1.1\r\n' \
              f'{host}\r\n' \
              f'Cookie: {cookie1}\r\n\r\n'
    else:  # both cookies given
        msg = f'GET {path} HTTP/1.1\r\n' \
              f'{host}\r\n' \
              f'Cookie: {cookie1}; {cookie2}\r\n\r\n'

    sock.send(msg.encode())


def receive_msg(sock):
    msg = sock.recv(4096).decode()
    return msg


def cookie_jar(msg):
    msg = msg.split()
    # find index and store first cookie
    idx = msg.index("Set-Cookie:")
    first_cookie = msg[idx + 1].strip(";")

    try:  # try finding second cookie, if there is only one, then an error will occur
        msg = msg[idx + 1:]
        second_index = msg.index("Set-Cookie:")
        second_cookie = msg[second_index + 1].strip(";")
    except:  # error, meaning only 1 cookie exists
        return first_cookie

    return first_cookie, second_cookie


def middle_token(msg):
    msg = msg.split()
    index = msg.index('name="csrfmiddlewaretoken"')
    token = msg[index + 1]
    token = token.split("=")
    token = token[1].replace('"', '')
    token = token.replace(">", "")
    return token


def create_login_body(username, password, token):
    return f'username={username}&' \
           f'password={password}&' \
           f'csrfmiddlewaretoken={token}&' \
           'next=/fakebook'


def login_user(sock, path, host, body_len, body, cookie1, cookie2):
    post_msg = f'POST {path} HTTP/1.1\r\n' \
               f'{host}\r\n' \
               'Content-Type: application/x-www-form-urlencoded\r\n' \
               f'Content-Length: {body_len}\r\n' \
               f'Cookie: {cookie2}; {cookie1}\r\n\r\n' \
               f'{body}'
    sock.send(post_msg.encode())


'''
This method implements the basic web crawler for this program. 
We use the HTML Parser object to parse through the current URL in search for more URLs and/or secret flags until all 
secret flags are found for the user. 
Also accounts for and appropriately handles different errors received on parsing through pages.
'''


def start_crawling(msg, sock, host, cookie3, cookie4):
    parser = FakebookHTMLParser()
    # feed the response to get the first set of URLs in the queue
    parser.feed(msg)
    # keep crawling until all flags are found or queue is empty
    while len(crawl_pages) != 0:
        # get the first site to be visited
        curr = (crawl_pages.popleft())

        if curr not in visited_links:  # if URL has not been visited
            send_get_request(curr, sock, host, cookie1=cookie3, cookie2=cookie4)
            msg = receive_msg(sock)
            error = handle_error(msg)
            # if error code 2 - don't do anything
            if error == 2:
                pass
            # if error code 403 or 404 - abandon URL
            elif error == 4:
                visited_links.add(curr)
                continue
            # if 500 error code - add it back to the queue to be requested again
            elif error == 5:
                crawl_pages.append(curr)
                continue
            # if error code 302 - grab new URL and request again
            elif error == 3:
                redirected_url = handle_err3(msg)
                # add redirected URL to queue if location is present
                if redirected_url != "":
                    crawl_pages.append(redirected_url)

            # feed the msg to parser to search for more links and secret flags
            parser.feed(msg)
            # after the site has been parsed, add it to the set of visited links
            visited_links.add(curr)
            # if we have all the secret flags then end the program
            if len(secret_flags) == 5:
                break


'''
This method is designed to handle error 301 which is Moved Permanently: This is known as an HTTP redirect. 
The method extracts the URL from the server response and sends it back to the crawler.
'''


def handle_err3(msg):
    # break msg into three parts - header, spacing, body
    msg = msg.partition('\r\n\r\n')

    # store only the header
    header = msg[0]
    redirect_url = ""
    # search for redirected link
    for h in header.split('\r\n'):
        if h.startswith("Location: "):
            redirect_url = h.split()[1]

    return redirect_url


'''
This method returns the appropriate code values indicating the type of error occurred so that it can be handled 
appropriately. Different status codes are: 
200 - everything is okay.
301 - Moved Permanently: This is known as an HTTP redirect. 
Your crawler should try the request again using the new URL given by the server.
403 - Forbidden and 404 - Not Found: In this case, your crawler should abandon the URL that generated the error code.
500 - Internal Server Error: Indicates that the Server could not or would not handle the request from the client. 
In this case, your crawler should re-try the request for the URL until the request is successful. 
Our web server may randomly return this error code to your crawler.
'''


def handle_error(msg):
    msg = msg.split()[1]
    if msg == "200":
        return 2
    elif msg == "302":
        return 2
    elif msg == "301":
        return 3
    elif msg == "403":
        return 4
    elif msg == "404":
        return 4
    elif msg == "500" or "501":
        return 5


def main():
    host = "Host: project2.5700.network"
    root_path = "/"
    fakebook = "/fakebook/"
    login_path = "/accounts/login/?next=/fakebook/"

    # Parse the username and password from the command line
    username, password = parse_cmd_line()

    # Create TLS wrapped socket
    sock = create_socket()

    # get the root page
    send_get_request(root_path, sock, host)

    # check the received message
    msg = receive_msg(sock)

    # store session cookie
    cookie_1 = cookie_jar(msg)

    # send get request for login page
    send_get_request(login_path, sock, host, cookie1=cookie_1)

    # check message for login page
    msg = receive_msg(sock)

    # retrieving csrf cookie and middle token
    cookie2 = cookie_jar(msg)
    token = middle_token(msg)

    # creating login body for user
    body = create_login_body(username, password, token)
    body_len = len(body.encode())

    # login user
    login_user(sock, login_path, host, body_len, body, cookie_1, cookie2)
    msg = receive_msg(sock)

    # store new cookies
    cookie3, cookie4 = cookie_jar(msg)

    # send request to go to my fakebook page
    send_get_request(fakebook, sock, host, cookie1=cookie3, cookie2=cookie4)
    msg = receive_msg(sock)

    # start our crawler
    start_crawling(msg, sock, host, cookie3, cookie4)

    # close the socket - program end
    sock.close()


if __name__ == "__main__":
    main()
