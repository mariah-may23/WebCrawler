#!/usr/bin/env python3

import cgi
import socket
import sys
import ssl
from collections import deque
from html.parser import HTMLParser

visited_links = set()
crawl_pages = deque()
secret_flags = list()


class FakebookHTMLParser(HTMLParser):
    is_h2 = False
    accumulated_data = ''

    def handle_starttag(self, tag, attrs):

        if tag == 'a':  # only looking at the links tags
            for attrs in attrs:  # attrs will be the href and the link in a tuple
                # print(attrs)
                link = attrs[1]  # the fakebook link or accounts/logout link
                # print(link)
                if link.startswith("/fakebook"):  # only want to fakebook pages link
                    # visited_links.add(link)
                    crawl_pages.append(link)

        if tag == "h2":
            for attributes in attrs:
                if "secret_flag" in attributes:
                    self.is_h2 = True

    def handle_endtag(self, tag):
        if tag == 'h2':
            self.is_h2 = False
            if self.accumulated_data != '':
                flag = self.accumulated_data.split()
                # print(flag)
                secret_flags.append(flag[1])
                print(flag[1])
            self.accumulated_data = ''

    def handle_data(self, data):
        if self.is_h2:
            self.accumulated_data += data


def parse_cmd_line():
    args_cmd = sys.argv
    username = ""
    password = ""

    file_name = sys.argv[0]
    username = sys.argv[1]
    password = sys.argv[2]

    return username, password


def create_socket():
    port = 443

    # http is based on TCP so create a TCP/IP socket
    # building connection

    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        context = ssl.create_default_context()
        sock = socket.create_connection(('project2.5700.network', port))
        wrapped_socket = context.wrap_socket(sock, server_hostname='project2.5700.network')
        return wrapped_socket
    except socket.error:
        sys.exit("Connection error")


def send_get_request(path, sock, host, cookie1=None, cookie2=None):
    if cookie1 is None and cookie2 is None:  # no cookies given
        msg = f'GET {path} HTTP/1.1\r\n' \
              f'{host}\r\n\r\n'
        # print(msg)
    elif cookie2 is None:  # only cookie 1 given
        msg = f'GET {path} HTTP/1.1\r\n' \
              f'{host}\r\n' \
              f'Cookie: {cookie1}\r\n\r\n'
    else:  # both cookies given
        msg = f'GET {path} HTTP/1.1\r\n' \
              f'{host}\r\n' \
              f'Cookie: {cookie1}; {cookie2}\r\n\r\n'

    # print(msg)
    sock.send(msg.encode())


def receive_msg(sock):

    msg = sock.recv(4096).decode()

    return msg


def cookie_jar(msg):
    msg = msg.split()
    # find index and store first cookie
    idx = msg.index("Set-Cookie:")
    first_cookie = msg[idx + 1].strip(";")

    try:  # try finding second cookie, if there is only one, then an error will occur
        msg = msg[idx + 1:]
        second_index = msg.index("Set-Cookie:")
        second_cookie = msg[second_index + 1].strip(";")
    except:  # error, meaning only 1 cookie exists
        return first_cookie

    return first_cookie, second_cookie


def middle_token(msg):
    msg = msg.split()
    # print(msg)
    index = msg.index('name="csrfmiddlewaretoken"')
    token = msg[index + 1]
    token = token.split("=")
    token = token[1].replace('"', '')
    token = token.replace(">", "")
    # print(token)
    return token


def create_login_body(username, password, token):
    return f'username={username}&' \
           f'password={password}&' \
           f'csrfmiddlewaretoken={token}&' \
           'next=/fakebook'


def login_user(sock, path, host, body_len, body, cookie1, cookie2):
    post_msg = f'POST {path} HTTP/1.1\r\n' \
               f'{host}\r\n' \
               'Content-Type: application/x-www-form-urlencoded\r\n' \
               f'Content-Length: {body_len}\r\n' \
               f'Cookie: {cookie2}; {cookie1}\r\n\r\n' \
               f'{body}'
    sock.send(post_msg.encode())


'''
This method implements the basic web crawler for this program. 
We use the HTML Parser object to parse through the current URL in search for more URLs and/or secret flags until all 
secret flags are found for the user. 
Also accounts for and appropriately handles different errors received on parsing through pages.
'''


def start_crawling(msg, sock, host, cookie3, cookie4):
    parser = FakebookHTMLParser()
    # feed the response to get the first set of URLs in the queue
    parser.feed(msg)
    # keep crawling until all flags are found or queue is empty
    while len(crawl_pages) != 0:
        # get the first site to be visited
        curr = (crawl_pages.popleft())

        if curr not in visited_links:
            send_get_request(curr, sock, host, cookie1=cookie3, cookie2=cookie4)
            msg = receive_msg(sock)
            error = handle_error(msg)
            # if error code 2 - don't do anything
            if error == 2:
                pass
            # if error code 403 or 404 - abandon URL
            elif error == 4:
                visited_links.add(curr)
                continue
            # if 500 error code - add it back to the queue to be requested again
            elif error == 5:
                crawl_pages.append(curr)
                continue
            # if error code 302 - grab new URL and request again
            elif error == 3:
                redirected_url = handle_err3(msg)
                # add redirected URL to queue if location is present
                if redirected_url != "":
                    crawl_pages.append(redirected_url)

            # feed the msg to parser to search for more links and secret flags
            parser.feed(msg)
            # after the site has been parsed, add it to the set of visited links
            visited_links.add(curr)
            # if we have all the secret flags then end the program
            if len(secret_flags) == 5:
                break


def handle_err3(msg):
    # break msg into three parts - header, spacing, body
    msg = msg.partition('\r\n\r\n')

    # store only the header
    header = msg[0]
    redirect_url = ""
    # search for redirected link
    for h in header.split('\r\n'):
        if h.startswith("Location: "):
            redirect_url = h.split()[1]

    return redirect_url


'''
This method returns the appropriate code values indicating the type of error occurred so that it can be handled 
appropriately. Different status codes are: 
200 - everything is okay.
301 - Moved Permanently: This is known as an HTTP redirect. 
Your crawler should try the request again using the new URL given by the server.
403 - Forbidden and 404 - Not Found: In this case, your crawler should abandon the URL that generated the error code.
500 - Internal Server Error: Indicates that the Server could not or would not handle the request from the client. 
In this case, your crawler should re-try the request for the URL until the request is successful. 
Our web server may randomly return this error code to your crawler.
'''


def handle_error(msg):
    msg = msg.split()[1]
    if msg == "200":
        return 2
    elif msg == "302":
        return 2
    elif msg == "301":
        return 3
    elif msg == "403":
        return 4
    elif msg == "404":
        return 4
    elif msg == "500" or "501":
        return 5


def main():
    host = "Host: project2.5700.network"
    root_path = "/"
    fakebook = "/fakebook/"
    login_path = "/accounts/login/?next=/fakebook/"

    # Parse the username and password from the command line
    username, password = parse_cmd_line()

    # Create TLS wrapped socket
    sock = create_socket()

    # get the root page
    send_get_request(root_path, sock, host)

    # check the received message
    msg = receive_msg(sock)

    # store session cookie
    cookie_1 = cookie_jar(msg)

    # send get request for login page
    send_get_request(login_path, sock, host, cookie1=cookie_1)

    # check message for login page
    msg = receive_msg(sock)

    # retrieving csrf cookie and middle token
    cookie2 = cookie_jar(msg)
    token = middle_token(msg)

    # creating login body for user
    body = create_login_body(username, password, token)
    body_len = len(body.encode())

    # login user
    login_user(sock, login_path, host, body_len, body, cookie_1, cookie2)
    msg = receive_msg(sock)

    # store new cookies
    cookie3, cookie4 = cookie_jar(msg)

    # send request to go to my fakebook page
    send_get_request(fakebook, sock, host, cookie1=cookie3, cookie2=cookie4)
    msg = receive_msg(sock)

    # start our crawler
    start_crawling(msg, sock, host, cookie3, cookie4)


    sock.close()


if __name__ == "__main__":
    main()
